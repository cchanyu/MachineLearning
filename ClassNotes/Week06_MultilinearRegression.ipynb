{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week06_MultilinearRegression",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXhaotNQxr2w"
      },
      "source": [
        "# Week 6\r\n",
        "# Multilinear Regression\r\n",
        "\r\n",
        "Last time we looked at a simple linear regression model $sales = \\beta_0 + \\beta_1\\cdot\\textit{TV advertising budget}$. More generally, a linear model makes a prediction by computing a weighted sum of their input features (plus a constant).\r\n",
        "\r\n",
        "**Reading: Chapter 4**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81lHyTpwx0zu"
      },
      "source": [
        "## Multilinear Regression: Model Assumptions\r\n",
        "**Model**:\r\n",
        "\r\n",
        "$\\hat{y} = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 +\\cdots + \\theta_nx_n$\r\n",
        "1. $\\hat{y}$ is the predicted value.\r\n",
        "2. $n$ is the number of features.\r\n",
        "3. $x_i$ is the i-th feature value.\r\n",
        "4. $\\theta_j$ is the j-th model parameter (associated with $x_j$)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azLxHqqUx02U"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb5RGmIdx04v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "ae42e172-14bb-4281-c95c-c24503b001de"
      },
      "source": [
        "# Toy example\r\n",
        "columns = ['Homework', 'Midterm', 'Final']\r\n",
        "data = pd.DataFrame({\r\n",
        "    \"Homework\": [95, 70, 80, 100, 70],\r\n",
        "    \"Midterm\": [90, 60, 80, 80, 85],\r\n",
        "    \"Final\": [93, 66, 85, 60, 90]\r\n",
        "}, index=[\"Alice\", \"Bob\", \"Clare\", \"David\", \"Eve\"])\r\n",
        "\r\n",
        "data.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Homework</th>\n",
              "      <th>Midterm</th>\n",
              "      <th>Final</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Alice</th>\n",
              "      <td>95</td>\n",
              "      <td>90</td>\n",
              "      <td>93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Bob</th>\n",
              "      <td>70</td>\n",
              "      <td>60</td>\n",
              "      <td>66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Clare</th>\n",
              "      <td>80</td>\n",
              "      <td>80</td>\n",
              "      <td>85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>David</th>\n",
              "      <td>100</td>\n",
              "      <td>80</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Eve</th>\n",
              "      <td>70</td>\n",
              "      <td>85</td>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Homework  Midterm  Final\n",
              "Alice        95       90     93\n",
              "Bob          70       60     66\n",
              "Clare        80       80     85\n",
              "David       100       80     60\n",
              "Eve          70       85     90"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKguV_HWx09V"
      },
      "source": [
        "In this case:\r\n",
        "- $x_1$ is the homework feature\r\n",
        "- $x_2$ is the midterm feature\r\n",
        "- $y$ is the final feature\r\n",
        "- model is: $final = \\theta_0 + \\theta_1 * homework + \\theta_2 * midterm$\r\n",
        "- We need to come up with values for $\\theta_0, \\theta_1, \\theta_2$ to complete the model.\r\n",
        "\r\n",
        "**Objective**: Suppose that another student Fred has Homework score 85 and Midterm score 80. What is prediction of his final exam score?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txFgiEHAx0_q"
      },
      "source": [
        "## Multilinear Regression: Vectorized form\r\n",
        "\r\n",
        "The multilinear model can also be written as:\r\n",
        "\r\n",
        "**$\\hat{y} = \\theta\\cdot\\textbf{x}$**.\r\n",
        "1. $\\theta = (\\theta_0, \\theta_1, ..., \\theta_n)$ is the paramter vector.\r\n",
        "2. $\\textbf{x} = (1, x_1, ..., x_n)$ is the feature vector.\r\n",
        "3. The symbol $\\cdot$ represents the inner-product of two vectors. For example, $(1, 2, 3)\\cdot (4, 5, 6) = 1\\times 4 + 2\\times 5 + 3\\times 6 = 32$.\r\n",
        "\r\n",
        "**Why is the expression $\\theta\\cdot\\textbf{x}$ equivalent to $\\theta_0 + \\theta_1x_1 + \\theta_2x_2 +\\cdots + \\theta_nx_n$?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8Acxyvfx1CF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e02253a-6118-45b5-8f2d-6be814876461"
      },
      "source": [
        "# Let's apply the linear regression tool in sci-kit learn on the toy example\r\n",
        "from sklearn.linear_model import LinearRegression\r\n",
        "\r\n",
        "model_lr = LinearRegression()\r\n",
        "model_lr.fit(data[[\"Homework\",\"Midterm\"]], data[\"Final\"]) \r\n",
        "# Input data requires two layers of brackets, because input features should be expressed as a list."
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoD9GMG1x1Em",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caa3b46a-8595-4817-80ea-f629aa32fa44"
      },
      "source": [
        "# Retrieve the estimated parameter values.\r\n",
        "print(\"Theta 0: \", model_lr.intercept_)\r\n",
        "print(\"Theta 1 to Theta n:\", model_lr.coef_)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Theta 0:  34.99999999999997\n",
            "Theta 1 to Theta n: [-0.71627907  1.30697674]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4DGOXEFHfTf",
        "outputId": "2985c1d5-474a-41c2-ca9e-1d793d1c23ab"
      },
      "source": [
        "# Apply the model to provide prediction for Fred\r\n",
        "model_lr.predict([[85, 80],\r\n",
        "                  [60, 60]])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([78.6744186 , 70.44186047])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vPsDL2-IGgQ",
        "outputId": "6df6e86f-3825-41ac-ed0c-ab91d058b0a6"
      },
      "source": [
        "# Remember prediction = theta0 + theta1 * homework + theta2 * midterm\r\n",
        "theta0 = 35\r\n",
        "theta1 = -0.716\r\n",
        "theta2 = 1.307\r\n",
        "\r\n",
        "prediction = theta0 + theta1 * 85 + theta2 * 80\r\n",
        "print(prediction)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "78.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlLIWR_1InUF",
        "outputId": "575bf85f-449b-45f8-de12-1c64cf003a19"
      },
      "source": [
        "# Let's use the vector form to get the prediction.\r\n",
        "# prediction = inner-product of the parameter vector and the feature vector.\r\n",
        "parameter_vector = np.array([35, -0.716, 1.307])\r\n",
        "feature_vector = np.array([1, 85, 80])\r\n",
        "\r\n",
        "prediction = parameter_vector.dot(feature_vector)\r\n",
        "print(prediction)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "78.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwJG44-7x1HE"
      },
      "source": [
        "## Multilinear Regression: Cost Function\r\n",
        "In order to calculate the best value for each parameter, we need a **cost function** that evaluates the errors made by a give set of parameter values. Here we use the **mean squared error (MSE)** function as the cost function:\r\n",
        "\r\n",
        "$J(\\textbf{X}, \\theta) = \\frac{1}{m}\\sum_{i=1}^{m}\\big(\\theta\\cdot\\textbf{x}^{(i)} - y^{(i)}\\big)^2$\r\n",
        "\r\n",
        "Here $(\\textbf{x}^{(i)}, y^{(i)})$ represents the i-th training example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uh-Zy_vEx1JZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ef6136d-1403-43ea-9a58-653003eea6d9"
      },
      "source": [
        "# Calculate the MSE cost of the toy example for the parameter values given by sci-kit learn.\r\n",
        "theta = np.array([35, -0.716, 1.307])\r\n",
        "list_errors = []\r\n",
        "\r\n",
        "for i in data.index:\r\n",
        "    x = np.array([1, data.loc[i, \"Homework\"], data.loc[i, \"Midterm\"]])\r\n",
        "    theta_dot_x = theta.dot(x)\r\n",
        "    y = data.loc[i, \"Final\"]\r\n",
        "    squared_error = (theta_dot_x - y) ** 2\r\n",
        "    list_errors.append(squared_error)\r\n",
        "\r\n",
        "print(list_errors)\r\n",
        "print(\"MSE:\",np.mean(list_errors))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[70.39210000000001, 7.290000000000015, 7.398399999999993, 63.361600000000124, 35.70062499999993]\n",
            "MSE: 36.82854500000002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZG2rW2SNx1Lw"
      },
      "source": [
        "## Multilinear Regression: Training Algorithm 1\r\n",
        "The value of $\\theta$ that minimizes the cost function is given by the following **normal equation**:\r\n",
        "\r\n",
        "$\\hat{\\theta} = \\big(\\textbf{X}^T\\cdot\\textbf{X}\\big)^{-1}\\cdot\\textbf{X}^T\\cdot\\textbf{y}$.\r\n",
        "\r\n",
        "1. $\\textbf{X}$ is an $m\\times (n+1)$ matrix whose i-th row is $\\textbf{x}^{(i)}$.\r\n",
        "$$\\textbf{X} = \\begin{pmatrix}\r\n",
        "1 & x^{(1)}_1 & x^{(1)}_2 & \\cdots & x^{(1)}_n \\\\\r\n",
        "1 & x^{(2)}_1 & x^{(2)}_2 & \\cdots & x^{(2)}_n \\\\\r\n",
        "\\vdots & \\vdots &\\vdots & \\ddots & \\vdots \\\\\r\n",
        "1 & x^{(m)}_1 & x^{(m)}_2 & \\cdots & x^{(m)}_n \\\\\r\n",
        "\\end{pmatrix}$$\r\n",
        "2. $$\\textbf{y} = \\begin{pmatrix}y^{(1)} \\\\ \\vdots \\\\ y^{(m)}\\end{pmatrix}$$.\r\n",
        "3. The cost function $J(\\theta)$ also has a matrix expression\r\n",
        "$$J(\\theta) = \\frac{1}{m}(\\textbf{X}^T\\cdot\\theta - \\textbf{y})^T\\cdot (\\textbf{X}^T\\cdot\\theta - \\textbf{y})$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0r86Sigx1OO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d2964f1-17ac-413d-bfec-a085d3840c1f"
      },
      "source": [
        "# Construct matrix X using np.hstack(), np.ones()\r\n",
        "\r\n",
        "# 1. Contruct a column of ones\r\n",
        "np.ones([5, 1])\r\n",
        "data[[\"Homework\", \"Midterm\"]].values\r\n",
        "X = np.hstack([np.ones([5, 1]), data[[\"Homework\", \"Midterm\"]].values])\r\n",
        "print(X)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  1.  95.  90.]\n",
            " [  1.  70.  60.]\n",
            " [  1.  80.  80.]\n",
            " [  1. 100.  80.]\n",
            " [  1.  70.  85.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIZZCIicx1Q7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9da5988a-3aff-4ec3-b44c-8f6cde8e2579"
      },
      "source": [
        "# Construct vector y\r\n",
        "y = data[[\"Final\"]].values\r\n",
        "print(y)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[93]\n",
            " [66]\n",
            " [85]\n",
            " [60]\n",
            " [90]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "197CDh19x1Th",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7057e92-d7be-4f88-e918-ec33854a7057"
      },
      "source": [
        "# Apply the normal equation to find theta\r\n",
        "# T = tranpose\r\n",
        "# dot = multiply\r\n",
        "# linalg = linear algebra\r\n",
        "# inv = inverse\r\n",
        "\r\n",
        "theta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\r\n",
        "print(\"Theta\", theta)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Theta [[35.        ]\n",
            " [-0.71627907]\n",
            " [ 1.30697674]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ezXjZlvx1WG"
      },
      "source": [
        "## Multilinear Regression: Training Algorithm 2\r\n",
        "The normal equation is not applicable when $\\textbf{X}^T\\cdot\\textbf{X}$ is not invertible. It happens if:\r\n",
        "- Several features are linearly dependent (for example, feature3 = feature1 + feature2)\r\n",
        "- The number of features is greater than the number of training data (for example, DNA data)\r\n",
        "\r\n",
        "When the matrix $\\textbf{X}$ is too large, the normal equation may take too long to finish since it requires a matrix multiplication.\r\n",
        "\r\n",
        "In these cases, we can use the **gradient descent** method to minimize the cost function instead.\r\n",
        "\r\n",
        "Gradient descent with one variable ideally looks like this:\r\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1600/0*fU8XFt-NCMZGAWND.\" width=\"600\">\r\n",
        "\r\n",
        "Gradient descent with two variables ideally looks like this:\r\n",
        "<img src=\"https://blog.paperspace.com/content/images/2019/09/F1-02.large.jpg\" width=\"600\">\r\n",
        "\r\n",
        "Gradient descent is an iterative algorithm for finding the **local minimum** of a differentiable function.\r\n",
        "- Choose an initial value of $\\hat{\\theta}$ and a **learning rate** $r$.\r\n",
        "- For each iteration $k$, do:\r\n",
        "$$\\hat{\\theta} \\leftarrow \\hat{\\theta} - r\\cdot\\frac{\\partial J(\\hat{\\theta})}{\\partial \\theta}.$$\r\n",
        "- The partial derivative of the cost function is given by\r\n",
        "$$\r\n",
        "\\frac{\\partial J(\\hat{\\theta})}{\\partial \\theta} = \\frac{2}{m}\\cdot\\textbf{X}^T\\cdot(\\textbf{X}\\cdot\\theta - \\textbf{y}).\r\n",
        "$$\r\n",
        "- **Verify the formula of partial derivative asuuming there is one input feature.**\r\n",
        "\r\n",
        "- End iteration if certain stop criteria is reached, such as:\r\n",
        "    - Value of $\\hat{\\theta}$ becomes stable.\r\n",
        "    - Certain iteration amount is reached."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clRpPwu-x1Yp"
      },
      "source": [
        "# Choose a random initial value for each parameter.\r\n",
        "\r\n",
        "theta = np.array([10, 1, 0.1]).reshape([3, 1])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yP5gFqHx1bM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd7118ad-135f-4e48-ae2b-12e38796858b"
      },
      "source": [
        "# Perform gradient descent once.\r\n",
        "# Choose a learning rate r\r\n",
        "r = 0.00001\r\n",
        "\r\n",
        "# 1. Calculate the gradient\r\n",
        "gradient = 2 / 5 * (X.T).dot(X.dot(theta) - y)\r\n",
        "print(\"gradient:\", gradient)\r\n",
        "\r\n",
        "# 2. Update the parameters\r\n",
        "theta = theta - r * gradient\r\n",
        "print(\"theta:\", theta)\r\n",
        "\r\n",
        "# 3. (optional) Show the MSE cost with new parameter values\r\n",
        "mat = X.dot(theta) - y\r\n",
        "MSE = 1/5 * mat.T.dot(mat)\r\n",
        "print(\"MSE:\",MSE)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gradient: [[  32.010736]\n",
            " [3023.293028]\n",
            " [2481.752064]]\n",
            "theta: [[9.99923789]\n",
            " [0.92924707]\n",
            " [0.04061248]]\n",
            "MSE: [[496.51640871]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPqDCTznx1dq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a6e65c2-6fc9-4d56-8106-f95d33461367"
      },
      "source": [
        "# Perform gradient descent multiple times\r\n",
        "# Start from scratch\r\n",
        "\r\n",
        "# Initialization of theta\r\n",
        "theta = np.array([10, 1, 0.1]).reshape([3, 1])\r\n",
        "r = 0.00006\r\n",
        "\r\n",
        "history = []\r\n",
        "\r\n",
        "for i in range(1000):\r\n",
        "    # Calculate the gradient with current value of theta\r\n",
        "    gradient = 2 / 5 * X.T.dot(X.dot(theta) - y)\r\n",
        "\r\n",
        "    # Update theta with the gradient\r\n",
        "    theta = theta - r * gradient\r\n",
        "\r\n",
        "    # (Optional) Calculate and display the MSE cost for the new theta\r\n",
        "    mat = X.dot(theta) - y\r\n",
        "    MSE = 1/5 * mat.T.dot(mat)\r\n",
        "    history.append(MSE[0, 0])\r\n",
        "\r\n",
        "print(\"Theta:\")\r\n",
        "print(theta)\r\n",
        "print(\"MSE:\", MSE)\r\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Theta:\n",
            "[[10.04660433]\n",
            " [-0.61165764]\n",
            " [ 1.50850399]]\n",
            "MSE: [[45.53771192]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sfCU6C33reK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "8438a788-c510-402a-fb3c-9739599e77cc"
      },
      "source": [
        "# Plot the learning curve.\r\n",
        "\r\n",
        "plt.plot(range(1000), history, 'b.')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fa975831310>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXOFqVwlx1gQ"
      },
      "source": [
        "**Discussion**\r\n",
        "1. Change $r$ to 0.000001 and 1. Observe the MSE curve.\r\n",
        "2. Do the initial parameter values matter?\r\n",
        "3. How to determine when to stop the iteration?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEnnJd4Qx1ii"
      },
      "source": [
        "1A: A larger learning rate will likely make the parameter go over the minimum. As a result, the learning curve will explode."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVWfuoUHx1lB"
      },
      "source": [
        "2A: The inital values determine the path of the gradient descent."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVfSS1dgx1np"
      },
      "source": [
        "3A: It depends the time we can afford.\r\n",
        "    If the learning curve becomes flat to a long time, it indicates that the MSE cannot be lower much more.\r\n",
        "    If the MSE value stablizes, It may imply that it has reached the minimum."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}